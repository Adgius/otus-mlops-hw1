{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "!pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7905cea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923c8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-23 12:38:58,519 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-08-23 12:38:58,600 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-08-23 12:38:58,600 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "Found 40 items\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2807409271 2022-12-05 09:23 s3a://mlops-data/fraud-data/2019-08-22.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2854479008 2022-12-05 09:24 s3a://mlops-data/fraud-data/2019-09-21.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2895460543 2022-12-05 09:27 s3a://mlops-data/fraud-data/2019-10-21.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2939120942 2022-12-05 09:30 s3a://mlops-data/fraud-data/2019-11-20.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995462277 2022-12-05 09:32 s3a://mlops-data/fraud-data/2019-12-20.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2994906767 2022-12-05 09:35 s3a://mlops-data/fraud-data/2020-01-19.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995431240 2022-12-05 09:38 s3a://mlops-data/fraud-data/2020-02-18.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995176166 2022-12-05 09:41 s3a://mlops-data/fraud-data/2020-03-19.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2996034632 2022-12-05 09:43 s3a://mlops-data/fraud-data/2020-04-18.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995666965 2022-12-05 09:47 s3a://mlops-data/fraud-data/2020-05-18.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2994699401 2022-12-05 09:50 s3a://mlops-data/fraud-data/2020-06-17.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995810010 2022-12-05 09:53 s3a://mlops-data/fraud-data/2020-07-17.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995995152 2022-12-05 09:56 s3a://mlops-data/fraud-data/2020-08-16.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995778382 2022-12-05 09:58 s3a://mlops-data/fraud-data/2020-09-15.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995868596 2022-12-05 10:01 s3a://mlops-data/fraud-data/2020-10-15.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995467533 2022-12-05 10:04 s3a://mlops-data/fraud-data/2020-11-14.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2994761624 2022-12-05 10:07 s3a://mlops-data/fraud-data/2020-12-14.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995390576 2022-12-05 10:10 s3a://mlops-data/fraud-data/2021-01-13.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995780517 2022-12-05 10:12 s3a://mlops-data/fraud-data/2021-02-12.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995191659 2022-12-05 10:15 s3a://mlops-data/fraud-data/2021-03-14.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 2995446495 2022-12-05 10:18 s3a://mlops-data/fraud-data/2021-04-13.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3029170975 2022-12-05 10:21 s3a://mlops-data/fraud-data/2021-05-13.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042691991 2022-12-05 10:24 s3a://mlops-data/fraud-data/2021-06-12.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3041980335 2022-12-05 10:27 s3a://mlops-data/fraud-data/2021-07-12.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042662187 2022-12-05 10:30 s3a://mlops-data/fraud-data/2021-08-11.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042455173 2022-12-05 10:33 s3a://mlops-data/fraud-data/2021-09-10.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042424238 2022-12-05 10:36 s3a://mlops-data/fraud-data/2021-10-10.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042358698 2022-12-05 10:39 s3a://mlops-data/fraud-data/2021-11-09.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042923985 2022-12-05 10:42 s3a://mlops-data/fraud-data/2021-12-09.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042868087 2022-12-05 10:45 s3a://mlops-data/fraud-data/2022-01-08.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3043148790 2022-12-05 10:48 s3a://mlops-data/fraud-data/2022-02-07.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3042312191 2022-12-05 10:50 s3a://mlops-data/fraud-data/2022-03-09.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3041973966 2022-12-05 10:54 s3a://mlops-data/fraud-data/2022-04-08.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3073760161 2022-12-05 10:56 s3a://mlops-data/fraud-data/2022-05-08.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3089378246 2022-12-05 10:59 s3a://mlops-data/fraud-data/2022-06-07.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3089589719 2022-12-05 11:03 s3a://mlops-data/fraud-data/2022-07-07.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3090000257 2022-12-05 11:07 s3a://mlops-data/fraud-data/2022-08-06.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3089390874 2022-12-05 11:10 s3a://mlops-data/fraud-data/2022-09-05.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3109468067 2022-12-05 11:13 s3a://mlops-data/fraud-data/2022-10-05.txt\n",
      "-rw-rw-rw-   1 ubuntu ubuntu 3136657969 2022-12-05 11:16 s3a://mlops-data/fraud-data/2022-11-04.txt\n",
      "2023-08-23 12:39:01,757 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-08-23 12:39:01,758 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-08-23 12:39:01,758 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls s3a://mlops-data/fraud-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e62dc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d54f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext()\n",
    "except ValueError:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "if sc:\n",
    "    sc.stop()\n",
    "    \n",
    "sc = SparkContext()\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"mytestapp\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a4acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "warnings.simplefilter('ignore')\n",
    "s3 = boto3.resource('s3',\n",
    "                endpoint_url='https://storage.yandexcloud.net',\n",
    "                region_name = 'ru-central1',\n",
    "                aws_access_key_id = 'YCAJEGyjDZ70buucdwAjU9B-c',\n",
    "                aws_secret_access_key = 'YCPfQDBIwyDGS-U0AHlXFZcn59PTI30ZP1tCpGN4'\n",
    "                )\n",
    "data_bucket = s3.Bucket('mlops-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde27f89",
   "metadata": {},
   "source": [
    "## Проверка подключения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1922137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 файлов\n"
     ]
    }
   ],
   "source": [
    "count = sum(1 for _ in data_bucket.objects.all())\n",
    "print(f'{count} файлов')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75117bb0",
   "metadata": {},
   "source": [
    "# Проверка корректности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfefb5",
   "metadata": {},
   "source": [
    "## 1. Проверка полноты данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0486f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data, cols, n_bytes = 10000):\n",
    "    txt = data.get().get('Body').read(n_bytes).decode()\n",
    "    header = txt.split('\\n')[0]\n",
    "    body = '\\n'.join(txt.split('\\n')[1:])\n",
    "    header = [i.strip() for i in header.replace('#', '').split('|')]\n",
    "    df= pd.read_csv(io.StringIO(body),sep=',', engine = 'python', names=header)\n",
    "    columns = df.columns.tolist()\n",
    "    for new_c in columns:\n",
    "        if new_c not in cols.keys():\n",
    "            print(f'Обнаружен новый столбец {new_c}')\n",
    "            cols[new_c] = dict()\n",
    "            cols[new_c].update({'dtype': [df[new_c].dtype],\n",
    "                                'NA': [df[new_c].isna().sum()],\n",
    "                                'nrows': [df.shape[0]]\n",
    "                                })\n",
    "        else:\n",
    "            cols[new_c]['dtype'].append(df[new_c].dtype)\n",
    "            cols[new_c]['NA'].append(df[new_c].isna().sum())\n",
    "            cols[new_c]['nrows'].append(df.shape[0])\n",
    "    for old_c in cols.keys():\n",
    "        if old_c not in columns:\n",
    "            print(f'{old_c} нет в списке столбцов!')\n",
    "    return cols # Dict[colname: [List of dtypes]]\n",
    "\n",
    "def read_csv(s3obj):\n",
    "    txt = s3obj.get().get('Body').read(1000).decode()\n",
    "    header = txt.split('\\n')[0]\n",
    "    body = '\\n'.join(txt.split('\\n')[1:])\n",
    "    header = [i.strip() for i in header.replace('#', '').split('|')]\n",
    "    df = sql.read.text(os.path.join(\"s3a://\" , i.bucket_name, i.key))\n",
    "    header_ = df.limit(1)\n",
    "    df = df.subtract(header_)\n",
    "    df = df.toDF(header)\n",
    "    return df\n",
    "\n",
    "def read_csv(s3obj):\n",
    "    #df = sql.read.text(os.path.join(\"s3a://\" , i.bucket_name, i.key))\n",
    "    df = sc.textFile(os.path.join(\"s3a://\" , i.bucket_name, i.key))\n",
    "    header =  df.first()\n",
    "    df = df.filter(lambda line: line != header)\n",
    "    header = [col.strip() for col in header.replace('#', '').split('|')]\n",
    "    schema = StructType([StructField(str(i), StringType(), True) for i in header])\n",
    "    return spark.createDataFrame(df, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d68266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_bucket.objects.all():\n",
    "    df = read_csv(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc79d5f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o346.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 96, rc1b-dataproc-d-ponxn1lyzug6w6o0.mdb.yandexcloud.net, executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 612, in prepare\n    verify_func(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1408, in verify\n    verify_value(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1395, in verify_struct\n    raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\nTypeError: StructType can not accept object '# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 612, in prepare\n    verify_func(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1408, in verify\n    verify_value(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1395, in verify_struct\n    raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\nTypeError: StructType can not accept object '# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-eb589bae8d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o346.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 96, rc1b-dataproc-d-ponxn1lyzug6w6o0.mdb.yandexcloud.net, executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 612, in prepare\n    verify_func(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1408, in verify\n    verify_value(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1395, in verify_struct\n    raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\nTypeError: StructType can not accept object '# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1692791613640_0005/container_1692791613640_0005_01_000008/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 612, in prepare\n    verify_func(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1408, in verify\n    verify_value(obj)\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1395, in verify_struct\n    raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\nTypeError: StructType can not accept object '# tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc2e6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql.read.text(os.path.join(\"s3a://\" , i.bucket_name, i.key))\n",
    "header =  df.limit(1)\n",
    "df = df.subtract(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "86a6c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|13928259,2019-08-...|\n",
      "|13928502,2019-08-...|\n",
      "|13928629,2019-08-...|\n",
      "|13928930,2019-08-...|\n",
      "|13929200,2019-08-...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "87740063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col.strip() for col in header.value.replace('#', '').split('|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08f9dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- # tranaction_id : string (nullable = true)\n",
      " |--  tx_datetime : string (nullable = true)\n",
      " |--  customer_id : string (nullable = true)\n",
      " |--  terminal_id : string (nullable = true)\n",
      " |--  tx_amount : string (nullable = true)\n",
      " |--  tx_time_seconds : string (nullable = true)\n",
      " |--  tx_time_days : string (nullable = true)\n",
      " |--  tx_fraud : string (nullable = true)\n",
      " |--  tx_fraud_scenario: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62678894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+-------------+-----------+-----------------+--------------+----------+------------------+\n",
      "|    # tranaction_id | tx_datetime | customer_id | terminal_id | tx_amount | tx_time_seconds | tx_time_days | tx_fraud | tx_fraud_scenario|\n",
      "+--------------------+-------------+-------------+-------------+-----------+-----------------+--------------+----------+------------------+\n",
      "|0,2019-08-22 06:5...|         null|         null|         null|       null|             null|          null|      null|              null|\n",
      "|1,2019-08-22 05:1...|         null|         null|         null|       null|             null|          null|      null|              null|\n",
      "|2,2019-08-22 19:0...|         null|         null|         null|       null|             null|          null|      null|              null|\n",
      "|3,2019-08-22 07:2...|         null|         null|         null|       null|             null|          null|      null|              null|\n",
      "|4,2019-08-22 09:0...|         null|         null|         null|       null|             null|          null|      null|              null|\n",
      "+--------------------+-------------+-------------+-------------+-----------+-----------------+--------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql.read.parquet(\"s3a://yc-mdb-examples/dataproc/example01/set01\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
