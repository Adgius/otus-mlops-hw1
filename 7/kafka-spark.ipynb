{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5111a745-a1eb-4731-8884-11d09ebf13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396092ca-0d43-4a89-86f0-bdd47f94c5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88af5c26-72cb-472c-9f22-879a8427133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ade78c2-bd6c-4a2b-8491-65b93683045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars org.apache.spark:spark-sql-kafka-0-10_2.12:2.0.0 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:2.0.0 pyspark-shell'\n",
    "# Создаем объект SparkSession, который является точкой входа в Spark SQL и Spark Streaming API.\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"MySparkApp\")\\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ee162-0718-48e8-8900-e0184d4a3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = '158.160.77.249'\n",
    "\n",
    "read_options = {\n",
    "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"mlops\" password=\"mlops_pw\";',\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\" : \"PLAINTEXT\",\n",
    "    \"kafka.bootstrap.servers\": f'{IP}:9092',\n",
    "    \"group.id\": 'test_group',\n",
    "    \"subscribe\": 'test',\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"mlops\" password=\"mlops_pw\";',\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\" : \"PLAINTEXT\",\n",
    "    \"kafka.bootstrap.servers\": f'{IP}:9092',\n",
    "    \"topic\": \"test\"\n",
    "}\n",
    "\n",
    "schema = t.StructType(\n",
    "    [\n",
    "        t.StructField('X', t.ArrayType(t.DoubleType()), True),\n",
    "        t.StructField('y', t.FloatType(), True),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a2c43-cbd3-405f-b733-dbe1bf2c692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_rows_from_json(df):\n",
    "    df = (df.selectExpr('CAST(value AS STRING)') \\\n",
    "      .select(from_json('value', schema).alias('raw_data')))\n",
    "\n",
    "    df = df.select('raw_data.y', *[col('raw_data.X').getItem(i).alias(f'X{i+1}') for i in range(0, 3)])\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_training_row_into_lp(df):\n",
    "    \n",
    "    features = Vectors.dense(row[\"x\"])\n",
    "    label = row[\"label\"]\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "\n",
    "def transform_test_row(row):\n",
    "    return Vectors.dense(row[\"x\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28a6ba6-d01e-4aa0-b953-bf1ebe8e8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=[\"X1\", \"X2\", \"X3\"], outputCol=\"features\")\n",
    "linear_regression = LinearRegression(featuresCol=\"features\", labelCol=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc0554-1935-44f7-9f22-b2465be056d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .options(**options) \\\n",
    "  .load()\n",
    "\n",
    "\n",
    "\n",
    "df = (df.selectExpr('CAST(value AS STRING)') \\\n",
    "      .select(from_json('value', schema).alias('raw_data')))\n",
    "\n",
    "df = df.select('raw_data.y', *[col('raw_data.X').getItem(i).alias(f'X{i+1}') for i in range(0, 3)])\n",
    "\n",
    "vectorized_data = vector_assembler.transform(df)\n",
    "\n",
    "model = linear_regression.fit(vectorized_data)\n",
    "\n",
    "stream_writer = vectorized_data.select().writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# # Запись предсказаний в Kafka\n",
    "# # stream_writer = predictions.writeStream \\\n",
    "# #     .format(\"kafka\") \\\n",
    "# #     .outputMode(\"append\") \\\n",
    "# #     .options(**write_kafka_params) \\\n",
    "# #     .start().awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "# Запуск потока обработки\n",
    "stream_writer.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "# Предсказание на потоковых данных с помощью модели\n",
    "predictions = model.transform(vectorized_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa061ffb-9388-443a-980c-6dcdd0ed1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .options(**read_options) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d19ff9c-fc90-498d-b155-aed47d4d9f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78e57ca2-fc2c-4c05-924e-ea167f697754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df schema\n",
      "root\n",
      " |-- raw_data: struct (nullable = true)\n",
      " |    |-- X: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- y: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (df.selectExpr('CAST(value AS STRING)') \\\n",
    "      .select(from_json('value', schema).alias('raw_data')))\n",
    "print(\"df schema\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9b4620a-4552-4b5c-8483-87af2b99be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(values):\n",
    "    # values = x.split(\",\")\n",
    "    features_ = []\n",
    "    for i in values:\n",
    "        features_.append(float(i))\n",
    "    features = Vectors.dense(features_)\n",
    "    return features\n",
    "\n",
    "extract_features_udf = udf(extract_features, VectorUDT())\n",
    "\n",
    "def extract_label(x):\n",
    "    values = x.split(\",\")\n",
    "    label = float(values[0])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a99c556-a52b-47bc-aa76-60d032b4d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"features\", extract_features_udf(col(\"raw_data.X\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54fe1f24-2e87-4411-9a4b-d9cd0412e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.select('raw_data.y', *[col('raw_data.X').getItem(i).alias(f'X{i+1}') for i in range(0, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed45ec3e-e412-4f0f-b6ae-02693302533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_data: struct (nullable = true)\n",
      " |    |-- X: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- y: float (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce2a353-29f5-4b5c-af6f-2cffd2170923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_assembler = VectorAssembler(inputCols=[\"X1\", \"X2\", \"X3\"], outputCol=\"features\")\n",
    "# vectorized_data = vector_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03c2baa0-7e79-4923-bb66-489bd97c6cd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-730c96f4bf63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlinear_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Предсказание на потоковых данных с помощью модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nkafka"
     ]
    }
   ],
   "source": [
    "# Создание и обучение модели линейной регрессии\n",
    "# vectorized_data = (vectorized_data\n",
    "#            .writeStream\n",
    "#            .format('console')\n",
    "#            .queryName('console-output')\n",
    "#            .start())\n",
    "\n",
    "linear_regression = LinearRegression(featuresCol=\"features\", labelCol=\"y\")\n",
    "model = linear_regression.fit(vectorized_data)\n",
    "\n",
    "# Предсказание на потоковых данных с помощью модели\n",
    "predictions = model.transform(vectorized_data)\n",
    "\n",
    "# Определение настроек для записи данных в Kafka\n",
    "\n",
    "write_kafka_params = {\n",
    "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"mlops\" password=\"mlops_pw\";',\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\" : \"PLAINTEXT\",\n",
    "    \"kafka.bootstrap.servers\": '158.160.70.184:9092',\n",
    "    \"topic\": \"test\"\n",
    "}\n",
    "\n",
    "# Запись предсказаний в Kafka\n",
    "stream_writer = predictions.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .options(**write_kafka_params) \\\n",
    "    .start().awaitTermination()\n",
    "\n",
    "# Запуск потока обработки\n",
    "stream_writer.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412c03d4-5418-4308-9edc-13909b46316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "names = [str(x) for x in np.random.choice([\"Alex\", \"James\", \"Michael\", \"Peter\", \"Harry\"], size=3)]\n",
    "ids = [int(x) for x in np.random.randint(1, 10, 3)]\n",
    "fruits = [str(x) for x in np.random.randint(1, 10, 3)]\n",
    "\n",
    "df = spark.createDataFrame(list(zip(names, ids, fruits)), [\"Name\", \"ID\", \"Fruit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f853c02-ecc6-4660-ad67-58bb5d77b05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([7.0, 1.0]), DenseVector([2.0, 3.0]), DenseVector([5.0, 5.0])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda row: Vectors.dense(row[1], row[2])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0dc32-4322-408b-81d6-21c3416662c7",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60476780-8b3b-430b-bddf-68ffa5c79de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .options(**read_options) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98dd0ddf-76b8-4115-985d-791d10dc79cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df schema\n",
      "root\n",
      " |-- raw_data: struct (nullable = true)\n",
      " |    |-- X: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- y: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (df.selectExpr('CAST(value AS STRING)') \\\n",
    "      .select(from_json('value', schema).alias('raw_data')))\n",
    "print(\"df schema\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e07d68d-eb83-4611-825c-d972b5ced014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- y: float (nullable = true)\n",
      " |-- X1: double (nullable = true)\n",
      " |-- X2: double (nullable = true)\n",
      " |-- X3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('raw_data.y', *[col('raw_data.X').getItem(i).alias(f'X{i+1}') for i in range(0, 3)])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32698ea-0c05-4a23-9f68-978510966aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=[\"X1\", \"X2\", \"X3\"], outputCol=\"features\")\n",
    "vectorized_data = vector_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cbe8b1-faf2-4b31-8a82-1f97847c4528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_batch(df, batch_id):\n",
    "    df.write \\\n",
    "      .format(\"parquet\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .save(\"df.parquet\")\n",
    "\n",
    "vectorized_data.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start() \\\n",
    "    .awaitTermination(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "615f088a-0ab8-467c-9146-9525c8566fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet(\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140fab67-e3f4-4e36-bd54-9f082175fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+-----+--------------------+\n",
      "|     y|                 X1|                 X2|   X3|            features|\n",
      "+------+-------------------+-------------------+-----+--------------------+\n",
      "| -54.0| 134.35635413628114| -91.79349863237272|-89.0|[134.356354136281...|\n",
      "|  -4.0| 18.489089918529782| 199.70239010752857|-81.0|[18.4890899185297...|\n",
      "| 117.0|  134.3184321122452|   268.522116453999|-41.0|[134.318432112245...|\n",
      "| 207.0|  175.8296964288368| 217.64853437525244| 49.0|[175.829696428836...|\n",
      "|  -9.0| -120.5086494891984|-137.85461564540003| 97.0|[-120.50864948919...|\n",
      "| -68.0| -95.15583628456142|-273.92613450221893| 73.0|[-95.155836284561...|\n",
      "|-121.0|   -45.951964528486|-253.29454266107354|-14.0|[-45.951964528486...|\n",
      "| -14.0| 182.54884467641264|-141.33374607420234|-53.0|[182.548844676412...|\n",
      "| 128.0|   148.122095766139| 204.58089576519555|-16.0|[148.122095766139...|\n",
      "|  48.0|-28.814188874385728| 208.00874692284435| -4.0|[-28.814188874385...|\n",
      "| -61.0|  -42.1244454445938| -92.40918256310277|-10.0|[-42.124445444593...|\n",
      "|-119.0|-161.92882078402448| -204.6105982248822| 29.0|[-161.92882078402...|\n",
      "| -25.0|-163.16337112763622| 19.631263020095375| 50.0|[-163.16337112763...|\n",
      "| -47.0|  57.87230691648316| -153.5395217402673|-22.0|[57.8723069164831...|\n",
      "|  41.0| -5.989523404296217| 210.23260628399385|-24.0|[-5.9895234042962...|\n",
      "|-114.0|-157.98156575051632| 1.9039234602025121|-36.0|[-157.98156575051...|\n",
      "| -23.0| -182.5250450176933| 245.12307449948528|-13.0|[-182.52504501769...|\n",
      "| -44.0| 11.194182721993132|  73.38908293156214|-71.0|[11.1941827219931...|\n",
      "| -36.0|  59.64751861372859|-14.191004113817371|-62.0|[59.6475186137285...|\n",
      "|  57.0|  60.71151421378394| 48.040915393982175|  6.0|[60.7115142137839...|\n",
      "+------+-------------------+-------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643ccc45-203f-4230-857a-e1142cc63a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(featuresCol=\"features\", labelCol=\"y\")\n",
    "model = linear_regression.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6408c-e224-4f4e-a0f6-4579077c73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на потоковых данных с помощью модели\n",
    "predictions = model.transform(vectorized_data)\n",
    "\n",
    "# Определение настроек для записи данных в Kafka\n",
    "\n",
    "write_kafka_params = {\n",
    "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"mlops\" password=\"mlops_pw\";',\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\" : \"PLAINTEXT\",\n",
    "    \"kafka.bootstrap.servers\": '158.160.70.184:9092',\n",
    "    \"topic\": \"test\"\n",
    "}\n",
    "\n",
    "stream_writer = (predictions\n",
    "           .writeStream\n",
    "           .format('console')\n",
    "           .queryName('console-output1')\n",
    "           .start())\n",
    "\n",
    "\n",
    "# Запись предсказаний в Kafka\n",
    "# stream_writer = predictions.writeStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .options(**write_kafka_params) \\\n",
    "#     .start().awaitTermination()\n",
    "\n",
    "# Запуск потока обработки\n",
    "stream_writer.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344f8e4-2a66-46b5-95c1-4c0030bc097e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
