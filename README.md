[Домашнее задание 1](#домашнее-задание-1)

[Домашнее задание 2](#домашнее-задание-2)

[Домашнее задание 3](#домашнее-задание-3)

[Домашнее задание 4](#домашнее-задание-4)

[Домашнее задание 5](#домашнее-задание-5)

# Домашнее задание 1

Цель проекта - разработать за полгода конкурентоспособную антифрод-систему.  Основная часть системы должна размещаться на внешних ресурсах. Система должна выдерживать высокие нагрузки. Также необходимо обеспечить конфиденциальность данных. За первые три месяца предоставить MVP.

Так как было определено, что у конкурентов максимум 2% мошеннических транзакций приводят к потере средств, то будем использовать метрику *False Negative Rate*:
$$FNR = \frac{FN}{FN+TP} * 100\% $$, где *FN* - число мошеннических транзакций, приведших к потере средств; *TP* - число распознаных мошеннических транзакций моделью. Данная метрика должна быть меньше 2%.

Для контроля недовольства клиентов – *False Positive Rate*. Доля ложных блокировок транзакций честного пользователя от общего числа транзакций.


$$FPR = \frac{FP}{FP+TN} * 100\%$$, где *FP* - число ложных срабатываний; *TN* - число обычных транзакций, которые распознаны как **не**мошеннические. Данная метрика должна быть меньше 5%.

Бизнес-метрика -- ущерб от мошеннических транзакций за месяц. Не должна превышать 500 тыс. рублей.

Особенности предстоящего проекта:

![image](https://github.com/Adgius/otus-mlops-hw1/assets/78685114/98e9fb05-7232-4f7a-a554-68f40fd44e61)

Основные фунциональные задачи:
1. Подготовка и сбор данных
2. Создание хранилища для данных
3. Разработка модели
4. Тесты
5. Деплой модели
6. Поддержка

# Домашнее задание 2

Основные команды:

**Создание бакета**

`yc storage bucket create --name otus-mlops-hw2 --default-storage-class cold --max-size 161061273600 --public-read --public-list`

Ссылка на s3: 

*s3://otus-mlops-hw2/fraud-data/*

**Копирование файлов с бакета на бакет**

`s3cmd cp -r --acl-public s3://mlops-data/fraud-data s3://otus-mlops-hw2/`

**Перемещение на hdfs**

`hadoop distcp s3a://otus-mlops-hw2/fraud-data/ /user/ubuntu/input/`

![image](https://github.com/Adgius/otus-mlops-hw1/assets/78685114/dfda7f39-5964-4100-b3ba-fb485d93ab6c)

Для поддержания работоспособности текущего сервиса (s3 + hdfs) необходимо ~26500 рублей
- S3 (150ГБ, холодное): ~250 руб/мес
- HDFS (мастер нода с публичным ip + дата нода 3 хоста) ~26000 руб/мес


В качестве оптимизации можно уменьшить число хостов в датаноде и использовать более дешевые конфигурации железа.

# Домашнее задание 3

**Создание бакета под очищенные данные**

`yc storage bucket create --name otus-mlops-data-clear --default-storage-class cold --max-size 161061273600 --public-read --public-list`

**Подключаемся к мастер ноде и запускаем jupyter notebook**

Чтобы в дальнейшем к нему подключиться, необходимо при создании кластера разрешить подключение на 8888 порт.

`jupyter notebook --no-browse --port 8888 --ip=*`

**Устанавливаем библиотеку для автоматического поиска зависимостей для Spark**

`pip install findspark`

**Исследуем данные**
[Ссылка на ноутбук](https://github.com/Adgius/otus-mlops-hw1/blob/master/3/data-exploration.ipynb)

**Составляем финальный скрипт**
[Ссылка на скрипт](https://github.com/Adgius/otus-mlops-hw1/blob/master/3/clean-data.py)

**Запускаем финальный скрипт для очистки**

В качестве аргументов передаем ключи от сервисного аккаунта s3.
`python clean-data.py [Идентификатор ключа] [Cекретный ключ]`

# Домашнее задание 4

### Периодический запуск процедуры очистки датасета мошеннических финансовых транзакций

**Для начала создаем ВМ, где будет развернут Ariflow. Рекомендуется минимум 8 ГБ оперативной памяти** 

**Подключаемся к ВМ по ssh**

**Клонируем этот репозиторий на ВМ**

`git clone https://github.com/Adgius/otus-mlops-hw1.git`

**Перемещаемся в папку с текущим дз**

`cd otus-mlops-hw1/4/`

**Устанавливаем docker на ВМ, с помощью подготовленного скрипта**

`bash install-docker.sh`

Чтобы изменения вступили в силу перезаходим в терминал

**Для подключения к Spark кластеру будет необходим ssh-ключ, поэтому создаем его**

`ssh-keygen`

**Чтобы ключ работал в Airflow, перемещаем его в рабочий проект**

`cp -R $HOME/.ssh $HOME/otus-mlops-hw1/4/`

**В директории с дз необходимо создать файл с окружением, в который поместим идентификатор пользователя**

`echo -e "AIRFLOW_UID=$(id -u)" > $HOME/otus-mlops-hw1/4/.env`

**Запустим инициализацию базы данных и создание пользователя**

Команды надо выполнять в директории `$HOME/otus-mlops-hw1/4`

`docker compose up airflow-init`

**Запустим Airflow**

`docker compose up -d`

Подключиться к интерфейсу можно через ip ВМ и порта 8080. Пароль и логин: `airflow`

При входе мы увидим такую картину:
![image](https://github.com/Adgius/otus-mlops-hw1/assets/78685114/f1fc824f-0647-45dd-b045-a548d9121940)

Один даг не добавился по причине отсутствия адреса Spark кластера, потому что он еще не создан. Подключение по ssh автоматически появится после успешного завершения первого дага

Для работы дагов требуется указать 4 переменные:
- `ssh-key` - это ssh ключ, который мы создали. Его можно узнать командой `cat $HOME/.ssh/id_rsa.pub`
- `yc_token` - OAuth токен. Его можно узнать на [этой странице](https://cloud.yandex.ru/docs/cli/quickstart)
- `aws_access_key_id` - публичная часть API-ключа от сервисного аккаунта с правами доступа к S3 хранилищу
- `aws_secret_access_key` - секретная часть API-ключа от сервисного аккаунта с правами доступа к S3 хранилищу

Запускаем первый даг и ждем, пока создастся кластер. Взаимодествие происходит через REST-API Яндекса, потому что утилиту `yc` не удалось поставить внутри контейнера с Airflow. Скрипт для создания кластера через `yc` можно найти по пути `otus-mlops-hw1/4/.data/create-cluster.sh`

После успешного завершения первого дага станет доступен второй даг `run_script`

Результат выполнения
![image](https://github.com/Adgius/otus-mlops-hw1/assets/78685114/240f6eb7-7257-4cfa-9529-ba8bd5683a7b)

# Домашнее задание 5

Основные компоненты остались как в дз 4. Что добавилось:
* В файл docker-compose добавил два сервиса: MLflow и базу PostgreSQL
* Добавил скрипт, который добавляет необходимые для запуска переменные окружения

Шаги для запуска (почти всё как в 4 задании):
1. `git clone https://github.com/Adgius/otus-mlops-hw1.git`
2. `ssh-keygen`
3. `cp -R $HOME/.ssh $HOME/otus-mlops-hw1/5/`
4. Добавляем с помощью команды `export` ключи от S3: AWS_ACCESS_KEY_ID и AWS_SECRET_ACCESS_KEY
6. Добавляем переменные окружения командой `bash otus-mlops-hw1/5/create-env.sh`
7. Устанавливаем docker `bash otus-mlops-hw1/5/install-docker.sh`
8. Перезаходим в терминал
9. `cd otus-mlops-hw1/5 ; docker compose up airflow-init`
10. `docker compose up -d`

Airflow доступен на 8080 порту, MLflow на 5000.

Заходим в Airflow, добавляем `yc_token` как в предыдущем дз и запускаем даг *create-spark-cluster*. Ждем окончания создания кластера.

Затем запускаем *run_script*.

По окончании процесса в логах mlflow мы увидим успешный ран с сохраненной моделью:
![image](https://github.com/Adgius/otus-mlops-hw1/assets/78685114/e8db599a-1f7d-4e79-b685-8a7887ad9c2a)

Что хочется отметить:
1. Метрика площадь под ROC-кривой 0.536... Но такая метрика обусловлена двумя причинами: долгий препроцессинг и ограниченность ресурсов. Стоит заметить на скриншоте, что скрипт выполнялся 45 мин. При этом из каждого файла с данными я брал только первые 100 000 строк, чтоб хоть как то ускорить процесс. Отсюда вытекает другая проблема - невозможность дождаться окончания работы скрипта по полном датасете в 2 млрд строк. Возможно проблему можно решить увеличив мощность железа, однако в силу ограничения по бюджету такой вариант кажется невозможным. Также не понятно на сколько нужно увеличить мощность, чтоб был положительный результат. Другим вариантом является оптимизация текущих процессов, но на данный момент нет знаний о том, как оптимально использовать оперативную память и файловую систему, "что можно и нужно делать, а что нельзя".
2. С точки зрения машинного обучения была взята простая логистическая регрессия и минимальный набор дополнительных фичей (описаны в классе *FeatureGenerator* внутри *run_pipeline.py*). Из-за того что некоторые фичи зависели от предыдущего времени, образовывались пропуски, которые заполнялись простым средним, что в свою очередь является не лучшим вариантом. Как можно улучшить: использовать больше данных (проблема описана выше), заменить модель, добавить больше фичей и выявить из них полезные (также отсылает к первому пункту) и более точечно подойти к проблеме холодного старта и пропуска.
3. Не удалось запустить скрипт через `spark-submit`, потому что он тянет python2. Но при этом на кластере по дефолту стоит python3, который я использовал для написания пайплайна внутри jupyter lab. При попытке запуска скрипта вылетала ошибка кодировки и ошибка наличия нужных библиотек. Мне кажется это из-за кастомного трансформера *FeatureGenerator*, который генерировал новые фичи. Кроме того, я не смог [сохранить целый пайплан](https://stackoverflow.com/questions/41399399/serialize-a-custom-transformer-using-python-to-be-used-within-a-pyspark-ml-pipel) внутри mlflow, потому что данная версия spark не поддерживает сериализацию данного класса.
4. Долго не мог понять, почему на датаноде нет ключей под s3, хотя при запуске mlflow я их указывал. Оказывается при обращению к серверу mlflow, он не предоставляет никаких секретов. Только дает возможность логирования в своей базе postgres. Поэтому ключи надо прокидывать через airlfow в локальное окружение при запуске скрипта в кластере.
